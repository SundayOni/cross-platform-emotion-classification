{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "md1",
      "metadata": {},
      "source": [
        "# Cross-Platform Emotion Classification — Preprocessing & EDA\n",
        "\n",
        "This notebook covers data loading, cleaning, text preprocessing, automated emotion annotation, and exploratory data analysis (EDA) for Twitter and Reddit mental health data.\n",
        "\n",
        "**Steps:**\n",
        "1. Load raw datasets\n",
        "2. Initial cleaning (duplicates, nulls, irrelevant content)\n",
        "3. Text preprocessing (emoji removal, tokenisation, lemmatisation)\n",
        "4. Emotion annotation using a pre-trained transformer model\n",
        "5. Exploratory Data Analysis (emotion distributions, text length, word counts, word clouds)\n",
        "6. Save cleaned datasets for modelling\n",
        "\n",
        "> **Note:** Raw data files are not included in this repository as they were collected via the Twitter and Reddit APIs and may contain sensitive content. To reproduce, collect data using `tweepy` (Twitter) and `praw` (Reddit) targeting mental health related keywords and subreddits."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md2",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print('All imports successful')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md3",
      "metadata": {},
      "source": [
        "## 2. Load Raw Data\n",
        "\n",
        "Data was collected via the Twitter API (using Tweepy) and Reddit API (using PRAW), targeting mental health related keywords and subreddits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "mh_reddit = pd.read_csv('../data/MHData_Reddit.csv')\n",
        "mh_twitter = pd.read_csv('../data/MHData_Twitter.csv')\n",
        "\n",
        "print(f'Reddit dataset:  {mh_reddit.shape[0]:,} rows, {mh_reddit.shape[1]} columns')\n",
        "print(f'Twitter dataset: {mh_twitter.shape[0]:,} rows, {mh_twitter.shape[1]} columns')\n",
        "\n",
        "print('\\nReddit columns:', mh_reddit.columns.tolist())\n",
        "print('Twitter columns:', mh_twitter.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "mh_reddit.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "mh_twitter.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md4",
      "metadata": {},
      "source": [
        "## 3. Initial Cleaning\n",
        "\n",
        "### 3a. Remove Duplicates and Irrelevant Columns\n",
        "\n",
        "- **Twitter:** Drop `user_id` and `created_at` (not needed for NLP)\n",
        "- **Reddit:** Drop metadata columns, rename `body` to `text` to standardise across platforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Twitter: remove duplicates and drop metadata columns\n",
        "mh_twitter_clean = mh_twitter.drop_duplicates(subset='text').drop(columns=['user_id', 'created_at'])\n",
        "\n",
        "# Reddit: remove duplicates, drop metadata, rename body to text\n",
        "mh_reddit_clean = mh_reddit.drop_duplicates(subset='body')\n",
        "mh_reddit_clean = mh_reddit_clean.drop(columns=['title', 'score', 'id', 'url', 'comms_num', 'created'])\n",
        "mh_reddit_clean = mh_reddit_clean.rename(columns={'body': 'text'})\n",
        "\n",
        "print(f'Twitter after deduplication: {mh_twitter_clean.shape}')\n",
        "print(f'Reddit after deduplication:  {mh_reddit_clean.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md5",
      "metadata": {},
      "source": [
        "### 3b. Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Twitter missing values:')\n",
        "print(mh_twitter_clean.isnull().sum())\n",
        "print('\\nReddit missing values:')\n",
        "print(mh_reddit_clean.isnull().sum())\n",
        "\n",
        "# Drop nulls from Reddit (Twitter has none)\n",
        "mh_reddit_clean = mh_reddit_clean.dropna()\n",
        "print(f'\\nReddit after dropping nulls: {mh_reddit_clean.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md6",
      "metadata": {},
      "source": [
        "### 3c. Remove Organisational and Governmental Content\n",
        "\n",
        "Some posts are from organisations (charities, government bodies, awareness campaigns) rather than individuals experiencing mental health issues. These are filtered out using a keyword list to ensure the dataset reflects genuine personal expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_keywords = [\n",
        "    'awareness', 'gov', 'official', 'ministry', 'our', 'organization', 'organisation',\n",
        "    'follow', 'donate', 'we need', 'government', 'startup', 'contest', 'initiative',\n",
        "    'department', 'movement', 'organize', 'organise', 'workshop', 'conference', 'seminar',\n",
        "    'study', 'research', 'services', 'tickets', 'suicideprevention', 'campus', 'program',\n",
        "    'reports', 'survey', 'statistics', 'industry', 'invited', 'join', 'series',\n",
        "    'register', 'enroll', 'launching', 'launch'\n",
        "]\n",
        "\n",
        "def contains_keywords(text, keywords):\n",
        "    for keyword in keywords:\n",
        "        if keyword.lower() in text.lower():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "mh_twitter_clean = mh_twitter_clean[~mh_twitter_clean['text'].apply(lambda x: contains_keywords(x, custom_keywords))]\n",
        "mh_reddit_clean  = mh_reddit_clean[~mh_reddit_clean['text'].apply(lambda x: contains_keywords(x, custom_keywords))]\n",
        "\n",
        "print(f'Twitter after keyword filtering: {mh_twitter_clean.shape}')\n",
        "print(f'Reddit after keyword filtering:  {mh_reddit_clean.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md7",
      "metadata": {},
      "source": [
        "## 4. Text Preprocessing\n",
        "\n",
        "Applying a standard NLP preprocessing pipeline to both datasets:\n",
        "- Remove emojis, URLs, mentions, special characters and numbers\n",
        "- Lowercase\n",
        "- Tokenise\n",
        "- Remove stopwords\n",
        "- Lemmatise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove Unicode emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\\U00002700-\\U000027BF\\U0001F900-\\U0001F9FF\"\n",
        "        \"\\U00002600-\\U000026FF\\U000025A0-\\U00002BEF]+\",\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)   # Remove special characters and numbers\n",
        "    text = text.lower()                         # Lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text) # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)            # Remove mentions\n",
        "    tokens = nltk.word_tokenize(text)           # Tokenise\n",
        "    tokens = [w for w in tokens if w not in stop_words]           # Remove stopwords\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens]            # Lemmatise\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print('Preprocessing function defined.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply to both datasets\n",
        "mh_twitter_clean['clean_text'] = mh_twitter_clean['text'].apply(preprocess_text)\n",
        "mh_reddit_clean['clean_text']  = mh_reddit_clean['text'].apply(preprocess_text)\n",
        "\n",
        "print('Sample before preprocessing:')\n",
        "print(mh_twitter_clean['text'].iloc[0])\n",
        "print('\\nSample after preprocessing:')\n",
        "print(mh_twitter_clean['clean_text'].iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md8",
      "metadata": {},
      "source": [
        "## 5. Emotion Annotation\n",
        "\n",
        "Using the pre-trained `bhadresh-savani/distilbert-base-uncased-emotion` model from Hugging Face to automatically label each post with one of six emotions: **joy, sadness, anger, fear, love, surprise**.\n",
        "\n",
        "Batch processing is used to handle large datasets efficiently. GPU is used if available, otherwise CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f'Using device: {\"GPU\" if device == 0 else \"CPU\"}')\n",
        "\n",
        "classifier = pipeline(\n",
        "    'text-classification',\n",
        "    model='bhadresh-savani/distilbert-base-uncased-emotion',\n",
        "    tokenizer='bhadresh-savani/distilbert-base-uncased-emotion',\n",
        "    truncation=True,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11",
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_batch(texts):\n",
        "    results = classifier(texts)\n",
        "    return [(r['label'], r['score']) for r in results]\n",
        "\n",
        "def apply_batch_processing(df, text_column, batch_size=16):\n",
        "    emotions, scores = [], []\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch = df[text_column].iloc[i:i+batch_size].tolist()\n",
        "        batch = [t if pd.notna(t) else '' for t in batch]\n",
        "        batch_results = classify_batch(batch)\n",
        "        batch_emotions, batch_scores = zip(*batch_results)\n",
        "        emotions.extend(batch_emotions)\n",
        "        scores.extend(batch_scores)\n",
        "    return pd.DataFrame({'emotion': emotions, 'score': scores})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This step is computationally expensive — expect several minutes on CPU\n",
        "mh_twitter_clean[['emotion', 'score']] = apply_batch_processing(mh_twitter_clean, 'clean_text')\n",
        "mh_reddit_clean[['emotion', 'score']]  = apply_batch_processing(mh_reddit_clean, 'clean_text')\n",
        "\n",
        "# Drop any rows where annotation failed\n",
        "mh_twitter_clean = mh_twitter_clean.dropna(subset=['emotion'])\n",
        "mh_reddit_clean  = mh_reddit_clean.dropna(subset=['emotion'])\n",
        "\n",
        "print(f'Twitter annotated: {mh_twitter_clean.shape}')\n",
        "print(f'Reddit annotated:  {mh_reddit_clean.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md9",
      "metadata": {},
      "source": [
        "## 6. Exploratory Data Analysis\n",
        "\n",
        "### 6a. Emotion Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "sns.countplot(x='emotion', data=mh_twitter_clean,\n",
        "              order=mh_twitter_clean['emotion'].value_counts().index, ax=axes[0])\n",
        "axes[0].set_title('Emotion Distribution — Twitter')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "sns.countplot(x='emotion', data=mh_reddit_clean,\n",
        "              order=mh_reddit_clean['emotion'].value_counts().index, ax=axes[1])\n",
        "axes[1].set_title('Emotion Distribution — Reddit')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Twitter emotion counts:')\n",
        "print(mh_twitter_clean['emotion'].value_counts())\n",
        "print('\\nReddit emotion counts:')\n",
        "print(mh_reddit_clean['emotion'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md10",
      "metadata": {},
      "source": [
        "### 6b. Text Length and Word Count Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14",
      "metadata": {},
      "outputs": [],
      "source": [
        "mh_twitter_clean['text_length'] = mh_twitter_clean['clean_text'].apply(len)\n",
        "mh_reddit_clean['text_length']  = mh_reddit_clean['clean_text'].apply(len)\n",
        "mh_twitter_clean['word_count']  = mh_twitter_clean['clean_text'].apply(lambda x: len(x.split()))\n",
        "mh_reddit_clean['word_count']   = mh_reddit_clean['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "sns.histplot(mh_twitter_clean['text_length'], bins=50, kde=True, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Twitter — Text Length Distribution')\n",
        "axes[0, 0].set_xlabel('Character Count')\n",
        "\n",
        "sns.histplot(mh_reddit_clean['text_length'], bins=50, kde=True, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Reddit — Text Length Distribution')\n",
        "axes[0, 1].set_xlabel('Character Count')\n",
        "\n",
        "sns.histplot(mh_twitter_clean['word_count'], bins=50, kde=True, color='green', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Twitter — Word Count Distribution')\n",
        "axes[1, 0].set_xlabel('Word Count')\n",
        "\n",
        "sns.histplot(mh_reddit_clean['word_count'], bins=50, kde=True, color='green', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Reddit — Word Count Distribution')\n",
        "axes[1, 1].set_xlabel('Word Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "for platform, df in [('Twitter', mh_twitter_clean), ('Reddit', mh_reddit_clean)]:\n",
        "    print(f'{platform} — Text length: {df[\"text_length\"].min()}–{df[\"text_length\"].max()} chars | Word count: {df[\"word_count\"].min()}–{df[\"word_count\"].max()} words')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md11",
      "metadata": {},
      "source": [
        "### 6c. Word Clouds by Emotion\n",
        "\n",
        "Visualising the most frequent terms for each emotion class across both platforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_wordclouds(df, platform_name):\n",
        "    emotions = df['emotion'].unique()\n",
        "    for emotion in sorted(emotions):\n",
        "        text = ' '.join(df[df['emotion'] == emotion]['clean_text'])\n",
        "        if text.strip():\n",
        "            wc = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(wc, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.title(f'Word Cloud — {emotion} ({platform_name})')\n",
        "            plt.show()\n",
        "\n",
        "plot_wordclouds(mh_twitter_clean, 'Twitter')\n",
        "plot_wordclouds(mh_reddit_clean, 'Reddit')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "md12",
      "metadata": {},
      "source": [
        "## 7. Save Cleaned Datasets\n",
        "\n",
        "Saving the annotated and cleaned datasets for use in the modelling notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16",
      "metadata": {},
      "outputs": [],
      "source": [
        "mh_twitter_clean.to_csv('../data/mh_twitter_clean.csv', index=False)\n",
        "mh_reddit_clean.to_csv('../data/mh_reddit_clean.csv', index=False)\n",
        "\n",
        "print(f'Saved Twitter: {mh_twitter_clean.shape[0]:,} rows')\n",
        "print(f'Saved Reddit:  {mh_reddit_clean.shape[0]:,} rows')\n",
        "print('Columns:', mh_twitter_clean.columns.tolist())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
