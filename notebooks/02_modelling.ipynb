{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.9"}
 },
 "cells": [
  {
   "cell_type": "markdown", "metadata": {}, "id": "md1",
   "source": ["# Cross-Platform Emotion Classification — Modelling\n\nThis notebook builds and evaluates machine learning models for emotion classification using preprocessed Twitter and Reddit mental health data.\n\n**Steps:**\n1. Load cleaned datasets\n2. TF-IDF vectorisation and label encoding\n3. Class imbalance handling (random oversampling)\n4. Single-platform evaluation (Twitter vs Twitter, Reddit vs Reddit)\n5. Cross-platform evaluation (train on one, test on the other)\n6. Results summary and key insights"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md2",
   "source": ["## 1. Imports"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c1",
   "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('All imports successful')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md3",
   "source": ["## 2. Load Cleaned Data"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c2",
   "source": ["mh_twitter = pd.read_csv('../data/mh_twitter_clean.csv')\nmh_reddit  = pd.read_csv('../data/mh_reddit_clean.csv')\n\n# Drop any rows with missing clean_text\nmh_twitter = mh_twitter.dropna(subset=['clean_text'])\nmh_reddit  = mh_reddit.dropna(subset=['clean_text'])\n\nprint(f'Twitter: {mh_twitter.shape}')\nprint(f'Reddit:  {mh_reddit.shape}')\nprint('\\nEmotion classes:', sorted(mh_twitter['emotion'].unique()))"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md4",
   "source": ["## 3. TF-IDF Vectorisation\n\nConverting text to numerical features using TF-IDF with unigrams and bigrams (`ngram_range=(1,2)`). English stopwords are removed.\n\nFor single-platform evaluation, each dataset is vectorised independently. For cross-platform evaluation, the vectoriser is fitted on combined data so both platforms share the same feature space."]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c3",
   "source": ["# Single-platform vectorisation (fit separately)\nvectorizer_twitter = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\nvectorizer_reddit  = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n\nX_twitter = vectorizer_twitter.fit_transform(mh_twitter['clean_text'])\nX_reddit  = vectorizer_reddit.fit_transform(mh_reddit['clean_text'])\n\nprint(f'Twitter TF-IDF matrix: {X_twitter.shape}')\nprint(f'Reddit TF-IDF matrix:  {X_reddit.shape}')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md5",
   "source": ["## 4. Label Encoding"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c4",
   "source": ["label_encoder = LabelEncoder()\n\ny_twitter = mh_twitter['emotion']\ny_reddit  = mh_reddit['emotion']\n\ny_twitter_encoded = label_encoder.fit_transform(y_twitter)\ny_reddit_encoded  = label_encoder.transform(y_reddit)\n\nprint('Emotion label mapping:')\nfor i, cls in enumerate(label_encoder.classes_):\n    print(f'  {i} → {cls}')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md6",
   "source": ["## 5. Train/Test Split and Oversampling\n\nSplitting each dataset 80/20. Oversampling is applied **only to the training set** using `RandomOverSampler` to balance class distribution without leaking test data."]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c5",
   "source": ["X_tw_train, X_tw_test, y_tw_train, y_tw_test = train_test_split(\n    X_twitter, y_twitter_encoded, test_size=0.2, random_state=42)\n\nX_rd_train, X_rd_test, y_rd_train, y_rd_test = train_test_split(\n    X_reddit, y_reddit_encoded, test_size=0.2, random_state=42)\n\nros = RandomOverSampler(random_state=42)\nX_tw_train_res, y_tw_train_res = ros.fit_resample(X_tw_train, y_tw_train)\nX_rd_train_res, y_rd_train_res = ros.fit_resample(X_rd_train, y_rd_train)\n\nprint(f'Twitter train (after oversampling): {X_tw_train_res.shape}')\nprint(f'Reddit train (after oversampling):  {X_rd_train_res.shape}')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md7",
   "source": ["### Class Distribution Before and After Oversampling"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c6",
   "source": ["fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\nfor ax, (before, after, title) in zip(axes, [\n    (np.bincount(y_tw_train), np.bincount(y_tw_train_res), 'Twitter'),\n    (np.bincount(y_rd_train), np.bincount(y_rd_train_res), 'Reddit')\n]):\n    n = len(before)\n    x = np.arange(n)\n    w = 0.35\n    ax.bar(x, before, w, label='Before', color='royalblue')\n    ax.bar(x + w, after[:n], w, label='After', color='orange')\n    ax.set_xticks(x + w/2)\n    ax.set_xticklabels(label_encoder.classes_, rotation=45)\n    ax.set_title(f'{title} — Class Distribution Before/After Oversampling')\n    ax.set_ylabel('Samples')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md8",
   "source": ["## 6. Helper Functions"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c7",
   "source": ["def evaluate_model(model, X_train, y_train, X_test, y_test, title):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(f'\\n=== {title} ===')\n    print(f'Accuracy: {acc:.4f}')\n    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n    return y_pred\n\ndef plot_confusion_matrix(y_true, y_pred, title, cmap='Blues'):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n                xticklabels=label_encoder.classes_,\n                yticklabels=label_encoder.classes_)\n    plt.title(title)\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.show()"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md9",
   "source": ["## 7. Single-Platform Evaluation\n\nTraining and testing each model on the same platform to establish baseline performance."]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md10",
   "source": ["### 7a. SVM — Twitter"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c8",
   "source": ["svm_twitter = SVC(kernel='linear', random_state=42)\ny_tw_pred_svm = evaluate_model(svm_twitter, X_tw_train_res, y_tw_train_res, X_tw_test, y_tw_test,\n                                'SVM — Twitter (Single-Platform)')\nplot_confusion_matrix(y_tw_test, y_tw_pred_svm, 'SVM Confusion Matrix — Twitter', cmap='Blues')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md11",
   "source": ["### 7b. SVM — Reddit"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c9",
   "source": ["svm_reddit = SVC(kernel='linear', random_state=42)\ny_rd_pred_svm = evaluate_model(svm_reddit, X_rd_train_res, y_rd_train_res, X_rd_test, y_rd_test,\n                                'SVM — Reddit (Single-Platform)')\nplot_confusion_matrix(y_rd_test, y_rd_pred_svm, 'SVM Confusion Matrix — Reddit', cmap='Reds')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md12",
   "source": ["### 7c. Naive Bayes — Twitter"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c10",
   "source": ["nb_twitter = MultinomialNB()\ny_tw_pred_nb = evaluate_model(nb_twitter, X_tw_train_res, y_tw_train_res, X_tw_test, y_tw_test,\n                               'Naive Bayes — Twitter (Single-Platform)')\nplot_confusion_matrix(y_tw_test, y_tw_pred_nb, 'Naive Bayes Confusion Matrix — Twitter', cmap='Purples')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md13",
   "source": ["### 7d. Naive Bayes — Reddit"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c11",
   "source": ["nb_reddit = MultinomialNB()\ny_rd_pred_nb = evaluate_model(nb_reddit, X_rd_train_res, y_rd_train_res, X_rd_test, y_rd_test,\n                               'Naive Bayes — Reddit (Single-Platform)')\nplot_confusion_matrix(y_rd_test, y_rd_pred_nb, 'Naive Bayes Confusion Matrix — Reddit', cmap='Greens')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md14",
   "source": ["## 8. Cross-Platform Evaluation\n\nTo test whether emotion patterns learned on one platform generalise to the other, we:\n1. Fit TF-IDF on **combined** Twitter + Reddit data (shared feature space)\n2. Train on one platform, test on the other\n\nThis is the core contribution of this project — assessing cross-platform generalisability."]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c12",
   "source": ["# Fit TF-IDF on combined data for shared feature space\ncombined_text = mh_twitter['clean_text'].tolist() + mh_reddit['clean_text'].tolist()\nvectorizer_combined = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\nvectorizer_combined.fit(combined_text)\n\nX_twitter_combined = vectorizer_combined.transform(mh_twitter['clean_text'])\nX_reddit_combined  = vectorizer_combined.transform(mh_reddit['clean_text'])\n\n# Fit label encoder on combined labels\ncombined_labels = mh_twitter['emotion'].tolist() + mh_reddit['emotion'].tolist()\nlabel_encoder_combined = LabelEncoder()\nlabel_encoder_combined.fit(combined_labels)\n\ny_twitter_enc = label_encoder_combined.transform(mh_twitter['emotion'])\ny_reddit_enc  = label_encoder_combined.transform(mh_reddit['emotion'])\n\n# Split into train/test\nX_tw_train_c, X_tw_test_c, y_tw_train_c, y_tw_test_c = train_test_split(\n    X_twitter_combined, y_twitter_enc, test_size=0.2, random_state=42)\n\nX_rd_train_c, X_rd_test_c, y_rd_train_c, y_rd_test_c = train_test_split(\n    X_reddit_combined, y_reddit_enc, test_size=0.2, random_state=42)\n\nprint(f'Combined vocabulary size: {len(vectorizer_combined.vocabulary_):,}')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md15",
   "source": ["### 8a. Train on Twitter, Test on Reddit"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c13",
   "source": ["svm_cross_tw = SVC(kernel='linear', class_weight='balanced', random_state=42)\nsvm_cross_tw.fit(X_tw_train_c, y_tw_train_c)\n\ny_rd_pred_cross = svm_cross_tw.predict(X_rd_test_c)\nprint('SVM — Train on Twitter, Test on Reddit:')\nprint(f'Accuracy: {accuracy_score(y_rd_test_c, y_rd_pred_cross):.4f}')\nprint(classification_report(y_rd_test_c, y_rd_pred_cross, target_names=label_encoder_combined.classes_))\n\nplot_confusion_matrix(y_rd_test_c, y_rd_pred_cross,\n                      'SVM — Train: Twitter | Test: Reddit', cmap='RdBu')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md16",
   "source": ["### 8b. Train on Reddit, Test on Twitter"]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c14",
   "source": ["svm_cross_rd = SVC(kernel='linear', class_weight='balanced', random_state=42)\nsvm_cross_rd.fit(X_rd_train_c, y_rd_train_c)\n\ny_tw_pred_cross = svm_cross_rd.predict(X_tw_test_c)\nprint('SVM — Train on Reddit, Test on Twitter:')\nprint(f'Accuracy: {accuracy_score(y_tw_test_c, y_tw_pred_cross):.4f}')\nprint(classification_report(y_tw_test_c, y_tw_pred_cross, target_names=label_encoder_combined.classes_))\n\nplot_confusion_matrix(y_tw_test_c, y_tw_pred_cross,\n                      'SVM — Train: Reddit | Test: Twitter', cmap='PiYG')"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md17",
   "source": ["## 9. Results Summary\n\nComparing all model configurations in a single table."]
  },
  {
   "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "id": "c15",
   "source": ["results = pd.DataFrame({\n    'Setup': [\n        'SVM — Twitter (Single)',\n        'SVM — Reddit (Single)',\n        'Naive Bayes — Twitter (Single)',\n        'Naive Bayes — Reddit (Single)',\n        'SVM — Train Twitter, Test Reddit (Cross)',\n        'SVM — Train Reddit, Test Twitter (Cross)'\n    ],\n    'Accuracy': [\n        accuracy_score(y_tw_test, y_tw_pred_svm),\n        accuracy_score(y_rd_test, y_rd_pred_svm),\n        accuracy_score(y_tw_test, y_tw_pred_nb),\n        accuracy_score(y_rd_test, y_rd_pred_nb),\n        accuracy_score(y_rd_test_c, y_rd_pred_cross),\n        accuracy_score(y_tw_test_c, y_tw_pred_cross)\n    ]\n})\n\nresults['Accuracy'] = results['Accuracy'].apply(lambda x: f'{x:.4f}')\nprint(results.to_string(index=False))"]
  },
  {
   "cell_type": "markdown", "metadata": {}, "id": "md18",
   "source": ["## 10. Key Findings\n\n- **SVM consistently outperforms Naive Bayes** across both single-platform and cross-platform setups\n- **Reddit shows more genuine mental health content** — sadness is the dominant emotion, reflecting the structured nature of mental health subreddits\n- **Twitter data is noisier** — joy dominates, likely due to hashtag misuse by non-affected users\n- **Cross-platform generalisation is limited** — models trained on one platform lose significant accuracy when tested on the other, highlighting the linguistic and contextual differences between platforms\n- **Sadness is the most cross-platform-detectable emotion** — it maintains relatively strong recall even in cross-platform settings, suggesting it has more consistent linguistic markers across both platforms\n- **Future improvements:** Fine-tuned transformer models (e.g. BERT, RoBERTa), better data filtering, and domain adaptation techniques could significantly improve cross-platform performance"]
  }
 ]
}
